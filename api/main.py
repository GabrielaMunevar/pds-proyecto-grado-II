"""
API REST para generación de Plain Language Summaries (PLS) médicos
Usando modelo T5-base fine-tuned

Autor: Proyecto de Grado - Maestría
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import logging
import time
import os

# Importar utilidades
from utils import (
    setup_chunking,
    generar_pls_con_chunking,
    calcular_todas_las_metricas,
    calcular_metricas_basicas,
    clasificar_texto
)

# Importar configuración
try:
    from config import settings, load_env_file
    # Cargar .env si existe
    load_env_file()
    # Configurar logging desde configuración
    log_level = getattr(logging, settings.LOG_LEVEL.upper(), logging.INFO)
    logging.basicConfig(level=log_level)
except ImportError:
    # Fallback si config.py no está disponible
    import os
    log_level = os.getenv("LOG_LEVEL", "INFO")
    logging.basicConfig(level=getattr(logging, log_level.upper(), logging.INFO))

logger = logging.getLogger(__name__)

# Crear app FastAPI
app = FastAPI(
    title="API de Generación PLS Médicos",
    description="API para generar Plain Language Summaries desde textos médicos técnicos usando T5-base",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS (permitir requests desde cualquier origen)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Montar archivos estáticos (frontend)
try:
    app.mount("/static", StaticFiles(directory="static"), name="static")
except:
    pass  # Si no existe la carpeta static, continuar sin ella

# ============================================================================
# MODELOS PYDANTIC (Request/Response)
# ============================================================================

class GenerateRequest(BaseModel):
    """Request to generate PLS"""
    technical_text: str = Field(..., description="Medical technical text to simplify", min_length=10)
    max_length: int = Field(256, description="Maximum output length in tokens", ge=50, le=512)
    num_beams: int = Field(4, description="Number of beams for generation", ge=1, le=10)
    
    class Config:
        schema_extra = {
            "example": {
                "technical_text": "Systemic arterial hypertension is a chronic medical condition characterized by persistent elevation of blood pressure...",
                "max_length": 256,
                "num_beams": 4
            }
        }

class GenerateResponse(BaseModel):
    """Response with generated PLS"""
    generated_pls: str
    generation_time: float
    num_chunks: int
    tokens_input: int
    tokens_output: int

class EvaluateRequest(BaseModel):
    """Request to evaluate PLS with metrics"""
    original_text: str = Field(..., description="Original technical text")
    generated_pls: str = Field(..., description="PLS generated by the model")
    reference_pls: Optional[str] = Field(None, description="Reference PLS (gold standard) for comparison")
    
    class Config:
        schema_extra = {
            "example": {
                "original_text": "Systemic arterial hypertension is a chronic medical condition...",
                "generated_pls": "High blood pressure is a health problem that lasts a long time...",
                "reference_pls": "High blood pressure is a chronic disease..."
            }
        }

class MetricsResponse(BaseModel):
    """Response with evaluation metrics"""
    # Similarity/overlap metrics
    rouge1: Optional[float] = None
    rouge2: Optional[float] = None
    rougeL: Optional[float] = None
    bleu: Optional[float] = None
    meteor: Optional[float] = None
    bertscore_f1: Optional[float] = None
    
    # Simplification metric
    sari: Optional[float] = None
    
    # Readability metrics
    flesch_reading_ease: float
    flesch_kincaid_grade: float
    
    # Compression metrics
    compression_ratio: float
    word_length: int
    original_word_length: int

class GenerateWithMetricsRequest(BaseModel):
    """Request to generate PLS and calculate metrics"""
    technical_text: str = Field(..., description="Medical technical text to simplify")
    reference_pls: Optional[str] = Field(None, description="Reference PLS for comparison")
    max_length: int = Field(256, ge=50, le=512)
    num_beams: int = Field(4, ge=1, le=10)

class GenerateWithMetricsResponse(BaseModel):
    """Response with PLS and metrics"""
    generated_pls: str
    metrics: MetricsResponse
    generation_time: float
    num_chunks: int

class ClassifyRequest(BaseModel):
    """Request to classify text as PLS or non-PLS"""
    text: str = Field(..., description="Text to classify", min_length=10)

class ClassifyResponse(BaseModel):
    """Response with classification result"""
    is_pls: bool
    confidence: float
    flesch_reading_ease: float
    flesch_kincaid_grade: float
    avg_word_length: float
    technical_terms_count: int
    reasoning: str

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    model_loaded: bool
    classifier_loaded: bool
    device: str
    model_path: str

# ============================================================================
# VARIABLES GLOBALES (Modelo cargado)
# ============================================================================

MODEL = None
TOKENIZER = None
TEXT_SPLITTER = None
DEVICE = None
MODEL_PATH = None

# ============================================================================
# STARTUP: CARGAR MODELO
# ============================================================================

@app.on_event("startup")
async def load_model():
    """Carga los modelos al iniciar la API"""
    global MODEL, TOKENIZER, TEXT_SPLITTER, DEVICE, MODEL_PATH
    
    logger.info("Starting PLS API...")
    
    # Try to download models from DVC/S3 if configured
    try:
        from utils_dvc import download_models_from_dvc_s3
        # Usar configuración centralizada si está disponible
        try:
            from config import settings
            dvc_bucket = settings.DVC_S3_BUCKET
            dvc_prefix = settings.DVC_S3_PREFIX
        except ImportError:
            # Fallback a os.getenv
            dvc_bucket = os.getenv("DVC_S3_BUCKET")
            dvc_prefix = os.getenv("DVC_S3_PREFIX", "dvcstore")
        
        if dvc_bucket:
            logger.info(f"Attempting to download models from DVC S3 (bucket: {dvc_bucket}/{dvc_prefix})...")
            download_models_from_dvc_s3(
                bucket_name=dvc_bucket,
                dvc_prefix=dvc_prefix
            )
    except ImportError:
        logger.debug("utils_dvc not available, using local models")
    except Exception as e:
        logger.warning(f"Could not download models from DVC S3: {e}. Using local models.")
    
    # Configurar device
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"Device: {DEVICE}")
    
    # ========================================================================
    # Cargar modelo T5 (Generador)
    # ========================================================================
    possible_paths = [
        "../models/t5_base",  # Desde api/
        "./models/t5_base",   # Desde api/ si está en la misma carpeta
        "models/t5_base",     # Relativo a donde se ejecuta
        os.path.join(os.path.dirname(os.path.dirname(__file__)), "models", "t5_base")  # Absoluto
    ]
    
    MODEL_PATH = None
    for path in possible_paths:
        if os.path.exists(path) and os.path.isdir(path):
            MODEL_PATH = path
            break
    
    if MODEL_PATH is None:
        raise FileNotFoundError(
            f"T5 model not found in any of these paths: {possible_paths}\n"
            "Please ensure the model is in models/t5_base/"
        )
    
    try:
        logger.info(f"Loading T5 model from: {MODEL_PATH}")
        TOKENIZER = AutoTokenizer.from_pretrained(MODEL_PATH)
        MODEL = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)
        MODEL.to(DEVICE)
        MODEL.eval()  # Evaluation mode
        
        logger.info(f"T5 model loaded successfully")
        logger.info(f"   Parameters: {sum(p.numel() for p in MODEL.parameters()):,}")
        
        # Configure text splitter for chunking
        TEXT_SPLITTER = setup_chunking(TOKENIZER)
        logger.info("Text splitter configured")
        
    except Exception as e:
        logger.error(f"Error loading T5 model: {e}")
        raise RuntimeError(f"Could not load T5 model: {e}")
    
    # ========================================================================
    # Load Classifier Model (optional, not critical)
    # ========================================================================
    try:
        from utils import load_classifier
        clf, vectorizer = load_classifier()
        if clf is not None and vectorizer is not None:
            logger.info("Classifier model loaded successfully")
        else:
            logger.warning("Classifier model not available, using heuristics for /classify")
    except Exception as e:
        logger.warning(f"Could not load classifier: {e}. Will use heuristics for /classify")

# ============================================================================
# ENDPOINTS
# ============================================================================

from fastapi.responses import FileResponse, HTMLResponse

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve the frontend application"""
    html_path = os.path.join("static", "index.html")
    if os.path.exists(html_path):
        with open(html_path, "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    return HTMLResponse(content="<h1>Medical PLS API</h1><p>Frontend not found. API is running at <a href='/docs'>/docs</a></p>")

@app.get("/api", response_model=Dict)
async def api_info():
    """API information endpoint"""
    return {
        "message": "Medical PLS Generation API",
        "version": "1.0.0",
        "documentation": "/docs",
        "endpoints": {
            "health": "/health",
            "classify": "/classify",
            "generate": "/generate",
            "evaluate": "/evaluate",
            "generate_with_metrics": "/generate-with-metrics"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check - verifies that the models are loaded"""
    from utils import CLASSIFIER, VECTORIZER
    
    return HealthResponse(
        status="ok" if MODEL is not None else "error",
        model_loaded=MODEL is not None,
        classifier_loaded=CLASSIFIER is not None and VECTORIZER is not None,
        device=DEVICE,
        model_path=MODEL_PATH or "N/A"
    )

@app.post("/generate", response_model=GenerateResponse)
async def generate_pls(request: GenerateRequest):
    """
    Generates a PLS from technical text
    
    - **technical_text**: Medical technical text to simplify
    - **max_length**: Maximum output length (default: 256)
    - **num_beams**: Number of beams for beam search (default: 4)
    """
    if MODEL is None or TOKENIZER is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        start_time = time.time()
        
        # Generate PLS
        generated_pls, num_chunks = generar_pls_con_chunking(
            texto=request.technical_text,
            model=MODEL,
            tokenizer=TOKENIZER,
            text_splitter=TEXT_SPLITTER,
            device=DEVICE,
            max_length=request.max_length,
            num_beams=request.num_beams
        )
        
        generation_time = time.time() - start_time
        
        # Count tokens
        tokens_input = len(TOKENIZER.encode(request.technical_text, add_special_tokens=False))
        tokens_output = len(TOKENIZER.encode(generated_pls, add_special_tokens=False))
        
        logger.info(f"PLS generated in {generation_time:.2f}s ({num_chunks} chunks)")
        
        return GenerateResponse(
            generated_pls=generated_pls,
            generation_time=round(generation_time, 3),
            num_chunks=num_chunks,
            tokens_input=tokens_input,
            tokens_output=tokens_output
        )
        
    except Exception as e:
        logger.error(f"Error in generation: {e}")
        raise HTTPException(status_code=500, detail=f"Error generating PLS: {str(e)}")

@app.post("/evaluate", response_model=MetricsResponse)
async def evaluate_pls(request: EvaluateRequest):
    """
    Calculates metrics for a generated PLS
    
    - **original_text**: Original technical text
    - **generated_pls**: PLS generated by the model
    - **reference_pls**: Reference PLS (optional, for similarity metrics)
    """
    try:
        # Calculate metrics
        if request.reference_pls:
            # Full metrics (with reference)
            metrics = calcular_todas_las_metricas(
                source=request.original_text,
                prediction=request.generated_pls,
                reference=request.reference_pls
            )
        else:
            # Basic metrics (without reference)
            metrics = calcular_metricas_basicas(
                source=request.original_text,
                prediction=request.generated_pls
            )
        
        logger.info(f"Metrics calculated")
        
        return MetricsResponse(**metrics)
        
    except Exception as e:
        logger.error(f"Error in evaluation: {e}")
        raise HTTPException(status_code=500, detail=f"Error calculating metrics: {str(e)}")

@app.post("/classify", response_model=ClassifyResponse)
async def classify_text(request: ClassifyRequest):
    """
    Classifies whether a text is a Plain Language Summary (PLS) or technical text
    
    - **text**: Text to classify
    
    Returns classification with confidence score and reasoning
    """
    try:
        result = clasificar_texto(request.text)
        logger.info(f"Text classified: is_pls={result['is_pls']}, confidence={result['confidence']:.3f}")
        return ClassifyResponse(**result)
    except Exception as e:
        logger.error(f"Error in classification: {e}")
        raise HTTPException(status_code=500, detail=f"Error classifying text: {str(e)}")

@app.post("/generate-with-metrics", response_model=GenerateWithMetricsResponse)
async def generate_with_metrics(request: GenerateWithMetricsRequest):
    """
    Generates PLS and calculates metrics in a single request
    
    - **technical_text**: Medical technical text to simplify
    - **reference_pls**: Reference PLS (optional)
    - **max_length**: Maximum output length (default: 256)
    - **num_beams**: Number of beams (default: 4)
    """
    if MODEL is None or TOKENIZER is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        start_time = time.time()
        
        # Generate PLS
        generated_pls, num_chunks = generar_pls_con_chunking(
            texto=request.technical_text,
            model=MODEL,
            tokenizer=TOKENIZER,
            text_splitter=TEXT_SPLITTER,
            device=DEVICE,
            max_length=request.max_length,
            num_beams=request.num_beams
        )
        
        generation_time = time.time() - start_time
        
        # Calculate metrics
        if request.reference_pls:
            metrics = calcular_todas_las_metricas(
                source=request.technical_text,
                prediction=generated_pls,
                reference=request.reference_pls
            )
        else:
            metrics = calcular_metricas_basicas(
                source=request.technical_text,
                prediction=generated_pls
            )
        
        logger.info(f"PLS + metrics in {generation_time:.2f}s")
        
        # Calculate token counts
        tokens_input = len(TOKENIZER.encode(request.technical_text))
        tokens_output = len(TOKENIZER.encode(generated_pls))
        
        response = GenerateWithMetricsResponse(
            generated_pls=generated_pls,
            metrics=MetricsResponse(**metrics),
            generation_time=round(generation_time, 3),
            num_chunks=num_chunks
        )
        
        # Add token counts to response (for frontend compatibility)
        response_dict = response.dict()
        response_dict['tokens_input'] = tokens_input
        response_dict['tokens_output'] = tokens_output
        
        return response_dict
        
    except Exception as e:
        logger.error(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # Cambiar a True en desarrollo
        log_level="info"
    )

