"""
API REST para generaci√≥n de Plain Language Summaries (PLS) m√©dicos
Usando modelo T5-base fine-tuned

Autor: Proyecto de Grado - Maestr√≠a
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import logging
import time
import os

# Importar utilidades
from utils import (
    setup_chunking,
    generar_pls_con_chunking,
    calcular_todas_las_metricas,
    calcular_metricas_basicas,
    clasificar_texto
)

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Crear app FastAPI
app = FastAPI(
    title="API de Generaci√≥n PLS M√©dicos",
    description="API para generar Plain Language Summaries desde textos m√©dicos t√©cnicos usando T5-base",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS (permitir requests desde cualquier origen)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Montar archivos est√°ticos (frontend)
try:
    app.mount("/static", StaticFiles(directory="static"), name="static")
except:
    pass  # Si no existe la carpeta static, continuar sin ella

# ============================================================================
# MODELOS PYDANTIC (Request/Response)
# ============================================================================

class GenerateRequest(BaseModel):
    """Request to generate PLS"""
    technical_text: str = Field(..., description="Medical technical text to simplify", min_length=10)
    max_length: int = Field(256, description="Maximum output length in tokens", ge=50, le=512)
    num_beams: int = Field(4, description="Number of beams for generation", ge=1, le=10)
    
    class Config:
        schema_extra = {
            "example": {
                "technical_text": "Systemic arterial hypertension is a chronic medical condition characterized by persistent elevation of blood pressure...",
                "max_length": 256,
                "num_beams": 4
            }
        }

class GenerateResponse(BaseModel):
    """Response with generated PLS"""
    generated_pls: str
    generation_time: float
    num_chunks: int
    tokens_input: int
    tokens_output: int

class EvaluateRequest(BaseModel):
    """Request to evaluate PLS with metrics"""
    original_text: str = Field(..., description="Original technical text")
    generated_pls: str = Field(..., description="PLS generated by the model")
    reference_pls: Optional[str] = Field(None, description="Reference PLS (gold standard) for comparison")
    
    class Config:
        schema_extra = {
            "example": {
                "original_text": "Systemic arterial hypertension is a chronic medical condition...",
                "generated_pls": "High blood pressure is a health problem that lasts a long time...",
                "reference_pls": "High blood pressure is a chronic disease..."
            }
        }

class MetricsResponse(BaseModel):
    """Response with evaluation metrics"""
    # Similarity/overlap metrics
    rouge1: Optional[float] = None
    rouge2: Optional[float] = None
    rougeL: Optional[float] = None
    bleu: Optional[float] = None
    meteor: Optional[float] = None
    bertscore_f1: Optional[float] = None
    
    # Simplification metric
    sari: Optional[float] = None
    
    # Readability metrics
    flesch_reading_ease: float
    flesch_kincaid_grade: float
    
    # Compression metrics
    compression_ratio: float
    word_length: int
    original_word_length: int

class GenerateWithMetricsRequest(BaseModel):
    """Request to generate PLS and calculate metrics"""
    technical_text: str = Field(..., description="Medical technical text to simplify")
    reference_pls: Optional[str] = Field(None, description="Reference PLS for comparison")
    max_length: int = Field(256, ge=50, le=512)
    num_beams: int = Field(4, ge=1, le=10)

class GenerateWithMetricsResponse(BaseModel):
    """Response with PLS and metrics"""
    generated_pls: str
    metrics: MetricsResponse
    generation_time: float
    num_chunks: int

class ClassifyRequest(BaseModel):
    """Request to classify text as PLS or non-PLS"""
    text: str = Field(..., description="Text to classify", min_length=10)

class ClassifyResponse(BaseModel):
    """Response with classification result"""
    is_pls: bool
    confidence: float
    flesch_reading_ease: float
    flesch_kincaid_grade: float
    avg_word_length: float
    technical_terms_count: int
    reasoning: str

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    model_loaded: bool
    device: str
    model_path: str

# ============================================================================
# VARIABLES GLOBALES (Modelo cargado)
# ============================================================================

MODEL = None
TOKENIZER = None
TEXT_SPLITTER = None
DEVICE = None
MODEL_PATH = None

# ============================================================================
# STARTUP: CARGAR MODELO
# ============================================================================

@app.on_event("startup")
async def load_model():
    """Carga el modelo al iniciar la API"""
    global MODEL, TOKENIZER, TEXT_SPLITTER, DEVICE, MODEL_PATH
    
    logger.info("üöÄ Iniciando API de PLS...")
    
    # Configurar device
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"üì± Device: {DEVICE}")
    
    # Ruta al modelo (ajusta seg√∫n tu estructura)
    # Intenta m√∫ltiples rutas posibles
    possible_paths = [
        "../models/t5_base",  # Desde api/
        "./models/t5_base",   # Desde api/ si est√° en la misma carpeta
        "models/t5_base",     # Relativo a donde se ejecuta
        os.path.join(os.path.dirname(os.path.dirname(__file__)), "models", "t5_base")  # Absoluto
    ]
    
    MODEL_PATH = None
    for path in possible_paths:
        if os.path.exists(path) and os.path.isdir(path):
            MODEL_PATH = path
            break
    
    if MODEL_PATH is None:
        raise FileNotFoundError(
            f"No se encontr√≥ el modelo en ninguna de estas rutas: {possible_paths}\n"
            "Por favor, aseg√∫rate de que el modelo est√© en models/t5_base/"
        )
    
    try:
        logger.info(f"üì• Cargando modelo desde: {MODEL_PATH}")
        TOKENIZER = AutoTokenizer.from_pretrained(MODEL_PATH)
        MODEL = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)
        MODEL.to(DEVICE)
        MODEL.eval()  # Modo evaluaci√≥n
        
        logger.info(f"‚úÖ Modelo cargado exitosamente")
        logger.info(f"   Par√°metros: {sum(p.numel() for p in MODEL.parameters()):,}")
        
        # Configurar text splitter para chunking
        TEXT_SPLITTER = setup_chunking(TOKENIZER)
        logger.info("‚úÖ Text splitter configurado")
        
    except Exception as e:
        logger.error(f"‚ùå Error al cargar modelo: {e}")
        raise RuntimeError(f"No se pudo cargar el modelo: {e}")

# ============================================================================
# ENDPOINTS
# ============================================================================

from fastapi.responses import FileResponse, HTMLResponse

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve the frontend application"""
    html_path = os.path.join("static", "index.html")
    if os.path.exists(html_path):
        with open(html_path, "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    return HTMLResponse(content="<h1>Medical PLS API</h1><p>Frontend not found. API is running at <a href='/docs'>/docs</a></p>")

@app.get("/api", response_model=Dict)
async def api_info():
    """API information endpoint"""
    return {
        "message": "Medical PLS Generation API",
        "version": "1.0.0",
        "documentation": "/docs",
        "endpoints": {
            "health": "/health",
            "classify": "/classify",
            "generate": "/generate",
            "evaluate": "/evaluate",
            "generate_with_metrics": "/generate-with-metrics"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check - verifies that the model is loaded"""
    return HealthResponse(
        status="ok" if MODEL is not None else "error",
        model_loaded=MODEL is not None,
        device=DEVICE,
        model_path=MODEL_PATH
    )

@app.post("/generate", response_model=GenerateResponse)
async def generate_pls(request: GenerateRequest):
    """
    Generates a PLS from technical text
    
    - **technical_text**: Medical technical text to simplify
    - **max_length**: Maximum output length (default: 256)
    - **num_beams**: Number of beams for beam search (default: 4)
    """
    if MODEL is None or TOKENIZER is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        start_time = time.time()
        
        # Generate PLS
        generated_pls, num_chunks = generar_pls_con_chunking(
            texto=request.technical_text,
            model=MODEL,
            tokenizer=TOKENIZER,
            text_splitter=TEXT_SPLITTER,
            device=DEVICE,
            max_length=request.max_length,
            num_beams=request.num_beams
        )
        
        generation_time = time.time() - start_time
        
        # Count tokens
        tokens_input = len(TOKENIZER.encode(request.technical_text, add_special_tokens=False))
        tokens_output = len(TOKENIZER.encode(generated_pls, add_special_tokens=False))
        
        logger.info(f"‚úÖ PLS generated in {generation_time:.2f}s ({num_chunks} chunks)")
        
        return GenerateResponse(
            generated_pls=generated_pls,
            generation_time=round(generation_time, 3),
            num_chunks=num_chunks,
            tokens_input=tokens_input,
            tokens_output=tokens_output
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error in generation: {e}")
        raise HTTPException(status_code=500, detail=f"Error generating PLS: {str(e)}")

@app.post("/evaluate", response_model=MetricsResponse)
async def evaluate_pls(request: EvaluateRequest):
    """
    Calculates metrics for a generated PLS
    
    - **original_text**: Original technical text
    - **generated_pls**: PLS generated by the model
    - **reference_pls**: Reference PLS (optional, for similarity metrics)
    """
    try:
        # Calculate metrics
        if request.reference_pls:
            # Full metrics (with reference)
            metrics = calcular_todas_las_metricas(
                source=request.original_text,
                prediction=request.generated_pls,
                reference=request.reference_pls
            )
        else:
            # Basic metrics (without reference)
            metrics = calcular_metricas_basicas(
                source=request.original_text,
                prediction=request.generated_pls
            )
        
        logger.info(f"‚úÖ Metrics calculated")
        
        return MetricsResponse(**metrics)
        
    except Exception as e:
        logger.error(f"‚ùå Error in evaluation: {e}")
        raise HTTPException(status_code=500, detail=f"Error calculating metrics: {str(e)}")

@app.post("/classify", response_model=ClassifyResponse)
async def classify_text(request: ClassifyRequest):
    """
    Classifies whether a text is a Plain Language Summary (PLS) or technical text
    
    - **text**: Text to classify
    
    Returns classification with confidence score and reasoning
    """
    try:
        result = clasificar_texto(request.text)
        logger.info(f"‚úÖ Text classified: is_pls={result['is_pls']}, confidence={result['confidence']:.3f}")
        return ClassifyResponse(**result)
    except Exception as e:
        logger.error(f"‚ùå Error in classification: {e}")
        raise HTTPException(status_code=500, detail=f"Error classifying text: {str(e)}")

@app.post("/generate-with-metrics", response_model=GenerateWithMetricsResponse)
async def generate_with_metrics(request: GenerateWithMetricsRequest):
    """
    Generates PLS and calculates metrics in a single request
    
    - **technical_text**: Medical technical text to simplify
    - **reference_pls**: Reference PLS (optional)
    - **max_length**: Maximum output length (default: 256)
    - **num_beams**: Number of beams (default: 4)
    """
    if MODEL is None or TOKENIZER is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        start_time = time.time()
        
        # Generate PLS
        generated_pls, num_chunks = generar_pls_con_chunking(
            texto=request.technical_text,
            model=MODEL,
            tokenizer=TOKENIZER,
            text_splitter=TEXT_SPLITTER,
            device=DEVICE,
            max_length=request.max_length,
            num_beams=request.num_beams
        )
        
        generation_time = time.time() - start_time
        
        # Calculate metrics
        if request.reference_pls:
            metrics = calcular_todas_las_metricas(
                source=request.technical_text,
                prediction=generated_pls,
                reference=request.reference_pls
            )
        else:
            metrics = calcular_metricas_basicas(
                source=request.technical_text,
                prediction=generated_pls
            )
        
        logger.info(f"‚úÖ PLS + metrics in {generation_time:.2f}s")
        
        return GenerateWithMetricsResponse(
            generated_pls=generated_pls,
            metrics=MetricsResponse(**metrics),
            generation_time=round(generation_time, 3),
            num_chunks=num_chunks
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # Cambiar a True en desarrollo
        log_level="info"
    )

